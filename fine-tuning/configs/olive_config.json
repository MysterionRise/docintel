{
  "$schema": "https://raw.githubusercontent.com/microsoft/Olive/main/olive/olive-schema.json",
  "description": "DocIntel ONNX quantisation config for WebGPU deployment (QInt4 weights, Float16 activations)",
  "input_model": {
    "type": "OnnxModel",
    "model_path": "./onnx_model"
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "accelerators": [
        {
          "device": "gpu",
          "execution_providers": [
            "WebGpuExecutionProvider"
          ]
        }
      ]
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [
            {
              "name": "avg"
            }
          ]
        }
      ]
    }
  },
  "passes": {
    "quantization": {
      "type": "OnnxMatMul4Quantizer",
      "config": {
        "block_size": 32,
        "is_symmetric": true
      }
    },
    "conversion": {
      "type": "OnnxFloatToFloat16",
      "config": {
        "keep_io_types": true
      }
    }
  },
  "pass_flows": [
    ["quantization", "conversion"]
  ],
  "host": "local_system",
  "target": "local_system",
  "output_dir": "./olive_model",
  "log_severity_level": 1
}
